# -*- coding: utf-8 -*-
"""kuee_ml_2020_colab_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SfUnj6oG5dtNdPnMlka5YmVyRxcN5xg5

ランタイム > ランタイムタイプの変更 > ハードウェアアクセラレータ
から，GPUを指定してください．

Google Driveをマウントするために，下のセルを実行してURLをクリックし，出てきたコードを貼り付けてEnterを押してください．
"""

import json
import os
import random

import numpy as np
import pandas as pd
import torch
from torch import nn
import torchvision
from google.colab import drive, files
from PIL import Image
from torchvision import transforms
drive.mount('/content/drive/', force_remount=True)

"""Google Driveからファイルをコピーします．
初回のみ10分程度時間がかかります．

"""

# ファイルコピー
if not os.path.exists("/content/kueeml2020_data_2/train_input.json"):
    !rsync --progress /content/drive/MyDrive/kuee-ml-2020/kueeml2020_data_2.tar /content/ 

    # rsync: change_dir "/content/drive/MyDrive/kuee-ml-2020" failed: No such file or directory (2) が出る場合，
    # まず，データセット申請のGoogleフォームに登録したメールアドレスでログインしていることを確認してください．
    # 次に，Googleフォームに登録したメールアドレスの受信ボックスを見て，Google Driveからのメールを見て開くボタンを押してください．
    # 次に，ColabにログインしているアカウントでGoogle Driveを開き，
    # 左の「共有アイテム」を押すと「kuee-ml-2020」があることを確認してください．
    # 次に，「マイドライブ」にkuee-ml-2020へのショートカットがあるか確認してください．
    # なければ，共有アイテムから「ドライブにショートカットを追加」でマイドライブにショートカットを作成してください． 
    # そして，画面左のファイル一覧で，drive/MyDrive/kuee-ml-2020 があることを確認してください． 

    # また，2行目を下の行に置き換えるとうまくいく場合があります．
    # !rsync --progress /content/drive/MyDrive/kuee_ml_2020/kueeml2020_data.tar /content/ 
    # それでもうまく行かない場合は，どこまで確認できたかと，スクリーンショットを添えて質問メールを送ってください．2021/1/17 追記

    # データを変更しました．
    # 2021/1/18 追記
    print("copied.")
    !tar -xvf /content/kueeml2020_data_2.tar
    print("extracted.")

device = torch.device('cuda')

seed = 0
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
if device=='cuda':
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
def worker_init_fn(worker_id):
    random.seed(worker_id)
    np.random.seed(worker_id)

# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
class MushroomDataset(torch.utils.data.Dataset):
    def __init__(self, json_file, root_dir, transform=None, is_train=False):
        assert os.path.exists(json_file), f"{json_file} does not exists"
        assert os.path.exists(root_dir), f"{root_dir} does not exists"
        with open(json_file) as f:
            self.annotations = json.load(f)
        self.root_dir = root_dir
        self.transform = transform
        self.is_train = is_train

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        ann = self.annotations[idx]
        file_path = os.path.join(self.root_dir, ann["file_path"])
        # sample = {"image": io.imread(file_path), "id": ann["id"]}
        image = Image.open(file_path)
        image_id = ann["id"]
        if self.is_train:
            label = ann["category_id"]
        else:
            label = -1
        image = self.transform(image)
        return image, image_id, label

BASE_PATH = "/content/kueeml2020_data"
TRAIN_JSON_PATH = os.path.join(BASE_PATH, "train_input.json")
TEST_JSON_PATH = os.path.join(BASE_PATH, "test_input.json")

preprocess = transforms.Compose([
    transforms.Resize(68),
    transforms.RandomCrop(64, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.Pad(10, fill=0, padding_mode='reflect'),
    transforms.RandomRotation(45, resample=False, expand=False, center=None, fill=(118, 109, 89)),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
    #transforms.Normalize(mean=[0.4555904, 0.42173338, 0.34482485], std=[0.23923218, 0.22198538, 0.22747745])
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

batch_size = 128
learning_rate = 0.001
num_classes = 100
# エポック数を少なくしました． 2021/01/18 追記
num_epochs = 30

train_ds = MushroomDataset(TRAIN_JSON_PATH, BASE_PATH, is_train=True, transform=preprocess)
test_ds = MushroomDataset(TEST_JSON_PATH, BASE_PATH, is_train=False, transform=preprocess)

train_dl = torch.utils.data.DataLoader(dataset=train_ds,
                                           batch_size=batch_size, 
                                           shuffle=True,
                                           num_workers=2,
                                           worker_init_fn=worker_init_fn)
test_dl = torch.utils.data.DataLoader(dataset=test_ds,
                                          batch_size=batch_size, 
                                          shuffle=False,
                                          num_workers=2,
                                          worker_init_fn=worker_init_fn)

class NeuralNet(torch.nn.Module):
    # 参考実装が間違っていました．すみません． 2021/01/17 追記
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.relu = nn.PReLU()
        self.dropout = nn.Dropout(p=0.5)
        self.dropout2 = nn.Dropout(p=0.3)
        self.maxpool = nn.MaxPool2d(kernel_size=2)
        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=2)
        self.conv3 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)
        self.conv4 = nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2)
        self.conv5 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)
        self.conv6 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)
        self.conv7 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)
        self.conv8 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)

        self.fc1 = nn.Linear(8192, 5000)
        self.fc2 = nn.Linear(5000, 1000)
        self.fc3 = nn.Linear(1000, 1000)
        self.fc4 = nn.Linear(1000, 100)


    def forward(self, x):
        out = self.conv1(x)
        out = self.relu(out)
        #out = self.conv2(out)
        #out = self.relu(out)
        out = self.maxpool(out)
        out = self.conv3(out)
        out = self.relu(out)
        #out = self.conv4(out)
        #out = self.relu(out)
        out = self.conv5(out)
        out = self.relu(out)
        #out = self.dropout2(out)
        out = self.maxpool(out)
        #out = self.conv6(out)
        #out = self.relu(out)
        out = self.conv7(out)
        out = self.relu(out)
        #out = self.conv8(out)
        #out = self.relu(out)
        out = self.maxpool(out)
        out = out.view(out.size(0), -1)
        out = self.fc1(out)
        out = self.relu(out)
        #out = self.dropout(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        out = self.relu(out)
        out = self.fc4(out)
        return out

model = NeuralNet()

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

model = model.to(device)
total_step = len(train_dl)
for epoch in range(num_epochs):
    for i, (images, image_ids, labels) in enumerate(train_dl):
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (i+1) % 5 == 0:
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))

model.eval()
with torch.no_grad():
    submission = []
    for i, (images, image_ids, _) in enumerate(test_dl):
        images = images.to(device)
        labels = labels.to(device)
        # Forward pass
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        for img_id, pred in zip(image_ids, predicted.cpu().detach()):
            submission.append((img_id.item(), pred.item()))

submission_file = "/content/submission.csv"
with open(submission_file, "w") as f:
    f.write("id,predicted\n")
    for sub in submission:
        f.write(f"{sub[0]},{sub[1]}\n")

files.download(submission_file)



